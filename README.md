In this paper, we focus on measuring and explaining the impact of adversarial examples on the performance of code vulnerability detection models. Our work mainly contains two parts: 1) We generalize eight synonymous transformation rules for constructing code synonymous adversarial examples, which have higher complexity and diversity with semantic preservation relative to the adversarial examples generated in previous work, to better measure the robustness of vulnerability detection models. 2) We explain the impact of code synonymous adversarial examples on the performance of vulnerability detection models in terms of two phases: code representations (input) and feature extraction (model). Experiments on publicly available datasets show that these vulnerability detection models drop 15.67\%-83.84\% and 9.95\%-86.26\% in Recall and F1 values, respectively. Through interpretable experiments, we found that the code synonymous adversarial examples change the abstract code structure originally used as input and the features extracted by the model and that the difference is the main reason for the degradation of the model's performance
